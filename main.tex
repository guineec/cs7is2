\documentclass{svproc}
\usepackage{url}
\def\UrlFont{\rmfamily}

\begin{document}
\mainmatter
\title{Ethical AI: A Survey of Proposed Solutions}
\subtitle{CS2IS2 Final Project (2017/2018)}
\author{Cian Guinee}

\institute{
\email{guineec@tcd.ie}}

\maketitle              % typeset the title of the contribution

\begin{abstract}
In early AI systems, agents had a relatively simple goal - solve the problem of their domain as efficiently as possible. This could be for example winning a game of chess, finding the shortest path between nodes on a graph or automating part of a factory assembly line. As the field has grown, however, the interactions between humans and intelligent agents have become increasingly close, and as a result problems have become much more complex. This has called into question, in many applications, what constitutes the goal for a system in this context. If we take the example of a self driving car: should the aim be to reach a destination as quickly as possible or should the emphasis be on safety, to the point where the car sits still? The answer, of course is a combination of both, and this presents a challenging problem for modern AI systems. This document looks at some of the ethical problems faced by modern AI systems, and reviews some of the proposed methods of addressing the open problems with them, making clear the advantages and disadvantages of each.
\keywords{Artificial Intelligence, ethics, automation, health}
\end{abstract}
%

% This document is a guideline for writing the final report for the CS7IS2 module Artificial Intelligence. You should follow its general structure as shown below.
% You should not change its format (font, size, margin, space, etc.). 
% Your report should be between five and ten pages. Report that not comply to the format or exceed the maximum length will be penalised (-5 marks).
% Brevity is desirable in communication, however you should provide all those details necessary for the good understanding of the described methods and algorithms. 
% The report will be graded on the basis of:
% \begin{itemize}
% \item Originality;
% \item Technical soundness;
% \item Organisation;
% \item Clarity of presentation;
% \item Adequacy of bibliography/Results (this last point strongly depends on the type of report)
% \end{itemize}
% You have two alternative choices for your report:
% \begin{description}
% \item[Survey.] This is a critical review of at least three papers that significantly contributed to advance the state-of-the-art for the problem you are analysing. This kind of report should not be a mere summary of the papers. You are expected to conduct an analytical review of the methods under analysis to try to find common aspect and differences, connections between methods, drawbacks and open problems. Unless the faced problem has emerged recently, students should choose their papers by diversifying the range of approaches used to solve the problem. A good guideline could be to choose a paper from a decade or two ago, and a couple of more recent papers. Also, try to include a paper that does not have the word deep in it. 
% \item[Original Report.] In this project you can propose your own research project. You can develop your own AI algorithm that solves a specific problem. 
% \end{description}

\section{Introduction}
% In this section, you should introduce your work: what are the motivations behind this work? What is the relevant problem that you are investigating? Why is it relevant? 
\par
The problem of building ethical artificial intelligence systems is one that has long been predicted, but has recently become more pressing. In early artificial intelligence, agents, most often, had clearly defined, single goals - from research applications in research, such as winning a game of chess to more practical goals, such as performing a task on an assembly line in a factory. Aside from agents having simpler goals, another important factor to take into account in the comparison of early and modern systems is their interaction with humans. Modern AI systems are growing ever closer to their users - if we look at an application that attempts to determine the state of a melanoma, for example \cite{kjoelen1995performance}, the impact on the user of the outcome, be it in a correct outcome, a false negative, or a false positive effects the human in the equation much more than in the case of earlier applications of AI. This, contrary to the aforementioned early AI systems, means that agents no longer have single purpose, clearly defined goals to determine what actions to take in a domain. Rather, they must take into account multiple metrics to determine what constitutes a goal, one of which is the ethical implications of their actions.
\par
This presents a challenge for developers of AI systems, as it is difficult to quantify good and bad in terms of ethical choices. The following survey delves into some of the work that has been done in this area recently, allowing the reader to get an understanding of which solutions have been proposed, and compares some of these proposed solutions to determine which, if any, has shown the most promising performance. The next section will outline some of the works studied in the compilation of this survey, and briefly introduce their contributions/proposed solutions.

\section{Related Work and Problem Definition}
\par
The following section will detail some of the work examined in the area described in the introduction, and outline both the literature that helped in description of the problem, and proposing solutions to this problem.

\par
One of the first works studied for this survey was \cite{anderson:anderson}, in which an extensive overview is given of both the problem at hand and the efforts in progress to solve it. First, this work describes some of the reasons this problem is pressing in modern AI. To this end, many insights are given into why there is a necessity to solve the problem of machine ethics. Initially, the paper lists many of the reasons already given in the introduction to this survey, which largely boil down to AI agents having a requirement to be aware of the effects they may have on the humans with which they interact. Further describing the problem and the need for practical solutions, the authors point out that aside from this, in order to be beneficial to its users, an AI system needs to be perceived as ethically aware in a way that aligns with these user's beliefs. This again presents a challenging problem, as humans do not have one agreed upon sense of what is ethical and what is not, rather the definition of what is ethically `correct' varies from person to person.
\par
In the work `Building Ethically Bounded AI \cite{rossi:mattei},' a similar description of the problem is given to that above, as well as some more examples which illustrate the need for AI to be `bounded' as the title would suggest. This paper also goes on to examine two of the most state-of-the-art proposed solutions for using ethics as a bounding metric for intelligent systems. The first solution proposed is to use CP-nets to model ethical priorities and preferences. CP-nets (\textbf{C}onditional \textbf{P}reference \textbf{Net}works), are a popular way to model preferences, used in a variety of applications. To explain simply, a CP-net comprises a series of `Conditional Preference Statements,' which define preference based upon some existing parameters - e.g. the preference of blue cars over red cars \cite{rossi:mattei}, which, when given new input, attempt to assign some preference to these items.
The example solution in this work, which comes from \cite{loreggia} uses separate CP-nets to model preferences and ethical guidelines to which these preferences can comply, as closely as possible.

\par
A second solution is proposed in \cite{balakrishnan} evaluates the incorporation of behavioural constraints in the ethical decision making process of AI systems. This, simply put, allows for certain well defined constraints to be used in the process of choosing actions by an agent. An example of this, given in \cite{kjoelen1995performance}, is the imposition of driving laws, such as speed limits, traffic lights etc. in the case of an agent in a self driving car system.
\par
Finally, in \cite{sirocco}, an outline of a framework for ethical decision making in AI systems, named SIROCCO, is given in which previous cases are considered in the process. In this solution, the problem of finding relevant cases to the problem in question is treated somewhat like an information retrieval problem. A language for describing cases is first outlined, in which cases are presented as a series of facts, with a set of attributes such as time, modifiers, duration over which the case occurred, for example. In every set of facts considered in the framework, one fact is considered the \textit{Questioned Fact}, which represents the ethical question that is being considered. Using this fact base, a hierarchy of facts is built and used in choosing actions for an agent.

\section{Proposed Solutions}
\subsection{CP-nets}
\par
CP-nets provide a way in which to model preference, and they achieve this by representing preference as a set of features, each with a corresponding set of parent features, which will impact the preference over the values in the feature set. This results in a so called \textit{dependency graph} being built \cite{loreggia}, where every node is preceded by its parents.
\par
The solution for the problem of ethics in AI defined in \cite{loreggia} uses CP-nets to model both preference and ethical principles. Using these separate nets, an idea of how closely aligned the defined ethical ideals and the modelled preferences are can be obtained, and through realignment of these ethical principals and preferences, an ethically aware agent can be produced. Also defined in \cite{loreggia} are several suggestions for a measure of distance in the context of CP-nets, which allow, in this application, appropriate ethical choices to be made. Choices are made through a pre-defined tolerance threshold in the distance between the principles CP-net and the preferences CP-net. A preference whose distance from the principles is within this threshold is deemed to be acceptable.

\subsection{Behavioural Constraints}
\par
The work done in \cite{balakrishnan}, the solution proposed uses a \textit{contextual multi-armed bandit} approach, in which a `context' is provided to the agent in the form of a feature vector, and selects an \textit{arm}, in this case a certain action. A \textit{regret} value is then determined, taking into account the context with which the agent was provided.
\par
Using the values observed for each arm, the model is updated by determining the expected reward from choosing an arm, and then ascertaining the actual reward received by taking that action. Using these values, the agents beliefs can be updated. In practice, this solution uses a reinforcement learning approach, consisting of two phases.
\par
Phase one is the learning phase, in which the model is trained with regard to the constraints imposed in the problem definition. To start, the agent makes decisions without regard for the context of which it must be aware. It makes recommendations, which are then evaluated by the teacher, which provides information to the agent on whether the recommended actions are allowed. Using this feedback, the agent updates its model to incorporate the constraints into its decision making process. With repetition, the agent will improve its performance, suggesting actions with high reward, which conform to the rules set out by the provided context.
\par
In phase two, the online recommendation phase, the model developed in the first step is put into action in the agent to provide sets of actions for new problems. It will use the MAB approach to choose an action, which corresponds to an arm in the MAB approach, the results of which are used to update relevant parameters and choose a final action.


\subsection{SIROCCO}
\par
Finally, the SIROCCO solution, defined in \cite{sirocco}, incorporates previous cases into the decision for the final action to be taken by the agent.
\par
First, a hierarchy of previous cases is constructed, taking into account cases relevant to the problem in question. A score for relevance is assigned to each case, after which the top N cases are chosen, relevant to the ethical question being posed.
\par
Using this top N relevant cases, several heuristics are used to determine the most relevant principles for the problem are chosen. Using these principles, a set of suggested actions are chosen as solutions to the problem.

\section{Experimental Results}
\subsection{CP-nets}
\par 
The experiments carried out in \cite{loreggia} were performed as follows: 1000 CP-nets are generated based on pre-defined values. Using these CP-nets, tests are carried out using the measure for distance defined by this work, and compared with a known distance measure, with the aim to find out if the new measure of distance was any better than previous solutions in representing a model for making decisions in an artificial system. The scenarios provided to the tests attempt to address both the outcome when an ideal choice, that is one that aligns perfectly with the ethical principles, is made, and situations where an ideal choice can not be made.
\par
In the results of these tests, it can be observed that when a higher tolerance is given to the system, more True Positives, that is where ethical choices are very close to in line with ethical principles, are observed often. Decreasing the threshold will increase the true negatives, that is situations where the choice is not deemed ethical and a compromise must be chosen. With regard to how easily a decision can be chosen, it emerges that choosing an action becomes easier with a higher threshold between choices.
\par
These results are to be expected - with more freedom in making choices, that is a looser constraint on how close a decision must be to the defined ethical principles, the number of adequate decisions will be higher, however the number of unsatisfactory outcomes in terms of a real world application, will also be higher. Similarly, where a tighter constraint is imposed, the number of potentially adequate actions for an agent to take which will be rejected is higher.
\par
These trade-offs are quite important in the context of an AI system, as an agent must both choose actions that align with the ethics of the humans with which it interacts, but also must be able to find compromise if an exact match can not be determined. As expected, the best model for ethical decision making based on CP-nets will be one that can find a balance between these values, although overall the model presented in this work does seem to be a plausible solution to the problem of modelling ethics, though while it may provide a good basic model for the decision making process, it would benefit from other metrics being used in deciding which actions to take, some of which could potentially include components of the other solutions discussed in this text.

\subsection{Behavioural Constraints}
\par
The experiments in \cite{balakrishnan} are carried out over a dataset containing information about a variety of films. In these tests, the constraint imposed was a certain age limit for films based on their content. The recommender would try to recommend films that align with the preferences of the individual in question, while staying away from films that do not meet the age constraints with regard to the individual involved. It is important to note that the dataset does not contain any data on age constraints for a certain film, so the recommender has no initial information in this regard.
\par
In experimental evaluation, the proposed solution produced convincing recommendations, reasonably compliant with the constraints provided. Again, however, this provides a solution only to one of the many problems that comprise the whole problem of ethical decision making in AI systems. While rules like this make a good starting point for such a system, adhering to rules takes no account for the `grey areas' that often come up in ethical questions.

\subsection{SIROCCO}
\par
In this experiment, comparisons between the proposed method and several other similar methods were compared on the same dataset - a set of 184 foundational cases and 58 trial cases. The foundational cases were transcribed to the machine understandable language outlined in previous sections, and used to train the model. Next, the trial cases were provided as input to the model, producing a set of relevant principles, to be used in taking an ethical action. F-Measure is used as a metric to compare the algorithms used in the experiment.
\par
In the experiments performed, SIROCCO is the clear favourite in all outcomes in suggesting relevant principles to the problem at hand. Its downfall, in this context, is that it lacks the production of a definitive result as to which action an agent should take, rather it defines which principles should be relevant in making ethical decisions.

\section{Conclusions}
The problem of ethical decision making in AI systems is one for which there is no single solution. The process of making these decisions can be improved by incorporating many different factors. As the problem continues to become more important to address, solutions will continually emerge, iterating on previous solutions and include findings from other works to build better answers to these difficult ethical questions into the future.

\begin{thebibliography}{5}
%
\bibitem{kjoelen1995performance}
Kjoelen, A., Thompson, M. J., Umbaugh, S. E., Moss, R. H., \& Stoecker, W. V. (1995). Performance of AI methods in detecting melanoma. IEEE Engineering in Medicine and Biology Magazine, 14(4), 411-416.

\bibitem {anderson:anderson}
Anderson, M., \& Anderson, S. L. (2007). Machine ethics: Creating an ethical intelligent agent. AI magazine, 28(4), 15-15.

\bibitem{rossi:mattei}
Rossi, F., \& Mattei, N. (2018). Building Ethically Bounded AI. arXiv preprint arXiv:1812.03980.


\bibitem {loreggia}
Loreggia, A., Mattei, N., Rossi, F., \& Venable, K. B. (2018, March). Preferences and ethical principles in decision making. In 2018 AAAI Spring Symposium Series.

\bibitem {balakrishnan}
Balakrishnan, A., Bouneffouf, D., Mattei, N., \& Rossi, F. (2018). Incorporating behavioral constraints in online ai systems. arXiv preprint arXiv:1809.05720.

\bibitem{sirocco}
McLaren, B. M. (2003). Extensionally defining principles and cases in ethics: An AI model. Artificial Intelligence, 150(1-2), 145-181.

\end{thebibliography}
\end{document}
